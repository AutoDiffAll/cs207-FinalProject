{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import automin.autodiff.variables as v\n",
    "import automin.autodiff.AD_numpy as anp\n",
    "import automin.autodiff.vector_variables as vv\n",
    "import automin.optimizer as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AutoMin: An Optimization Package Using Automatic Differentiation\n",
    "\n",
    "## Group 5: Team Non-CS Majors\n",
    "\n",
    "### Eric Choi, Josh Feldman, Youting Kou, Shane Ong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taking derivatives is an essential operation in numerical methods, optimization, and science. From a computational perspective, however, calculating a derivative can be difficult.\n",
    "\n",
    "**Finite Differences** requires careful selection of $\\epsilon$\n",
    "\n",
    "**Symbolic Differentiation** is infeasible for complicated functions, especially for higher order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "**Automatic Differentiation** overcomes these challenges by providing both quick and accurate derivatives.\n",
    "\n",
    "This also allows us to provide optimization methods that are fast and accurate. Most of the standard optimization modules such as `scipy.optimize` do not rely on automatic differentiation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Brief Mathematical Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/background.png)\n",
    "$\n",
    "y = x^2 + 1 \\\\ \n",
    "z = \\frac{1}{y} \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " We would like to obtain \n",
    "$\n",
    "\\frac{\\partial z}{\\partial x}\n",
    "$\n",
    "by using the chain rule:\n",
    "\n",
    "$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/background.png)\n",
    "$\n",
    "y = x^2 + 1 \\\\ \n",
    "z = \\frac{1}{y} \\\\\n",
    "$\n",
    "\n",
    "In this case, \n",
    "\n",
    "$\n",
    "\\frac{\\partial z}{\\partial y} = -\\frac{1}{y^2} = -\\frac{1}{(x^2+1)^2} \\\\\n",
    "\\frac{\\partial y}{\\partial x} = 2x\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/background.png)\n",
    "$\n",
    "\\frac{\\partial z}{\\partial y} = -\\frac{1}{y^2} = -\\frac{1}{(x^2+1)^2} \\\\\n",
    "\\frac{\\partial y}{\\partial x} = 2x\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore, \n",
    "\n",
    "$\n",
    "\\frac{\\partial z}{\\partial x} \\\\\n",
    "= \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x} \\\\ \n",
    "= -\\frac{2x}{(x^2+1)^2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo for AD module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The user provides a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f1 = lambda x, y: anp.exp(3*x) + anp.log(y/x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...the user can calculate its value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.83425860084327"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...but now the user wishes to get its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable name: f(a,b), Value: 403.83425860084327, Derivatives: {'a': 1209.7863804782053, 'b': 0.3333333333333333}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = v.Variable('a', 2)\n",
    "b = v.Variable('b', 3)\n",
    "res1 = f1(a,b)\n",
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo for AD module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similarly, if the user has a vector function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@vv.vectorize_variable\n",
    "def vec_fn(x, y, z):\n",
    "    f1 = x * y + anp.sin(y) + anp.cos(z)\n",
    "    f2 = x + y + anp.sin(x*y)\n",
    "    return np.array([f1,f2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...the user can extract the values, jacobian, and partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c = v.Variable('c', 2)\n",
    "res2 = vec_fn(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f0: Variable name: f(a,c,b), Value: 5.724973171512725, Derivatives: {'a': 3, 'c': -0.9092974268256817, 'b': 1.0100075033995546}\n",
       "f1: Variable name: f(a,b), Value: 4.720584501801074, Derivatives: {'a': 3.880510859951098, 'b': 2.920340573300732}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [5.72497317 4.7205845 ].\n",
      "Partial Derivative wrt a: [3.         3.88051086]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.010008</td>\n",
       "      <td>-0.909297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.880511</td>\n",
       "      <td>2.920341</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c\n",
       "0  3.000000  1.010008 -0.909297\n",
       "1  3.880511  2.920341  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Value: {}.'.format(res2.val))\n",
    "print('Partial Derivative wrt a: {}'.format(res2.partial_der(a)))\n",
    "display(res2.jacobian())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now all that seems trivial. \n",
    "\n",
    "When would a user just want to solely calculate the gradient of a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Instead, suppose a user wants to find the root of a given function using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def newton_method_scalar(fn, x, threshold, max_iter, verbose=True, norm=2):\n",
    "    \n",
    "    var_names = ['x'+str(idx) for idx in range(len(x))]\n",
    "    \n",
    "    x = np.array(x)\n",
    "    nums_iteration = 0\n",
    "    while True:\n",
    "        x_new = x - fn(*x) / get_grad(fn, x, var_names)\n",
    "\n",
    "        # print iteration output\n",
    "        if verbose is True:\n",
    "            print(f'Iteration at {nums_iteration}, at {x} ')\n",
    "        \n",
    "        # threshold stopping condition \n",
    "        if np.linalg.norm(x-x_new, norm) < threshold:\n",
    "            print(f'After {nums_iteration} iterations, found a root: {x_new}')\n",
    "            break\n",
    "        \n",
    "        # iteration stopping condition\n",
    "        if nums_iteration >= max_iter:\n",
    "            break\n",
    "        nums_iteration +=1\n",
    "        x = x_new\n",
    "        \n",
    "def get_grad(fn, x, var_names):\n",
    "    variables = [v.Variable(var_names[idx], x_n) for idx, x_n in enumerate(x)]\n",
    "    out = fn(*variables)\n",
    "    jacobian = out.jacobian()\n",
    "    grad = np.array([jacobian[name] for name in var_names])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration at 0, at [3 2 1] \n",
      "Iteration at 1, at [4.5 3.5 2.5] \n",
      "Iteration at 2, at [3.75 2.75 1.75] \n",
      "Iteration at 3, at [4.125 3.125 2.125] \n",
      "Iteration at 4, at [3.9375 2.9375 1.9375] \n",
      "Iteration at 5, at [4.03125 3.03125 2.03125] \n",
      "Iteration at 6, at [3.984375 2.984375 1.984375] \n",
      "Iteration at 7, at [4.0078125 3.0078125 2.0078125] \n",
      "Iteration at 8, at [3.99609375 2.99609375 1.99609375] \n",
      "Iteration at 9, at [4.00195312 3.00195312 2.00195312] \n",
      "Iteration at 10, at [3.99902344 2.99902344 1.99902344] \n",
      "Iteration at 11, at [4.00048828 3.00048828 2.00048828] \n",
      "Iteration at 12, at [3.99975586 2.99975586 1.99975586] \n",
      "Iteration at 13, at [4.00012207 3.00012207 2.00012207] \n",
      "Iteration at 14, at [3.99993896 2.99993896 1.99993896] \n",
      "Iteration at 15, at [4.00003052 3.00003052 2.00003052] \n",
      "Iteration at 16, at [3.99998474 2.99998474 1.99998474] \n",
      "Iteration at 17, at [4.00000763 3.00000763 2.00000763] \n",
      "Iteration at 18, at [3.99999619 2.99999619 1.99999619] \n",
      "Iteration at 19, at [4.00000191 3.00000191 2.00000191] \n",
      "Iteration at 20, at [3.99999905 2.99999905 1.99999905] \n",
      "Iteration at 21, at [4.00000048 3.00000048 2.00000048] \n",
      "Iteration at 22, at [3.99999976 2.99999976 1.99999976] \n",
      "After 22 iterations, found a root: [4.00000012 3.00000012 2.00000012]\n"
     ]
    }
   ],
   "source": [
    "f2 = lambda x, y, z: (x-4)**2 + (y-3)**2 + (z-2)**2\n",
    "newton_method_scalar(f2, [3, 2, 1], 1e-6, 50, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**It is relatively simple to write up code for algorithms requiring first-order derivatives!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**A common class of problems requiring derivatives is optimization**\n",
    "\n",
    "As such, we decided to use our AD module as a basis for an optimization package!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization\n",
    "\n",
    "Generally, optimization is to find the 'best available' values of an objective function given a defined domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simply, optimization can be thought of as maximizing (or minimizing) a real function**\n",
    "\n",
    "Our optimization module thus seeks to provide users a way to minimize functions. We have implemented 4 first-order iterative algorithms to minimize a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization\n",
    "\n",
    "1. Gradient Descent\n",
    "\n",
    "2. Steepest Descent\n",
    "\n",
    "3. Conjugate Gradient \n",
    "\n",
    "4. Broyden-Fletcher-Goldfarb-Shanno (BFGS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is the simplest first-order iterative algorithm but introduces a key idea that forms the basis of iterative algorithms. \n",
    "\n",
    "**Notice that if we walk in the negative direction of the gradient, it seems like we will eventually reach the minimum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Mathematically, for a function $\\ell(x)$ this is saying that to move from $x^{(t)}$ to $x^{(t+1)}$, we should follow the path of steepest descent $-\\nabla\\ell(x^{(t)})$\n",
    "\n",
    "If we do this over and over again ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/gradient-descent1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/gradient-descent2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/gradient-descent3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear that we will eventually reach the minimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, an even better method would be to update the direction with each step\n",
    "\n",
    "This will allow us to pick favorable directions that take better steps or to pick 'non-interfering' directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steepest Descent\n",
    "\n",
    "Steepest descent chooses a new direction through a line minimzation along a direction $s$, before moving along a new direction $-\\nabla f$. \n",
    "\n",
    "![steepestdescent.png](figs/steepestdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This basically means that steepest descent is picking a new direction that is orthogonal to the previous direction\n",
    "\n",
    "\n",
    "![descent.png](figs/descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient\n",
    "\n",
    "After the line minimization along a direction $u$, instead of moving along a direction $v = -\\nabla f$ which is the direction of steepest descent, we can instead perturb the direction to ensure that $\\nabla f$ is perpendicular to the line minimization direction $u$ before and after we move along the perturbed direction.\n",
    "\n",
    "![](figs/conjugategradient.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This ensures that the conjugate gradient method moves in non-interfering directions**\n",
    "\n",
    "In steepest descent, we only make right angle turns. Each step is along directions that can undo the minimization in the previous step. As such, we might take some time before it reaches the minimum. This is often the case in practice.\n",
    "\n",
    "Conjugate gradient instead uses conjugate directions which preserve the minimization achieved in the previous step. \n",
    "\n",
    "![](figs/descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When one is optimizing a function over large datasets, it can be costly to compute the gradient over all the data (even with automatic differentiation). Instead, one can evaluate the function for one datapoint at a time and make gradient descent updates for each evaluation. In the long run, this approximates gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One can implement linear regression with our automatic differentiation package as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indep_var1</th>\n",
       "      <th>indep_var2</th>\n",
       "      <th>dep_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.022755</td>\n",
       "      <td>-1.955844</td>\n",
       "      <td>-5.491880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.115004</td>\n",
       "      <td>0.179297</td>\n",
       "      <td>3.795530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.295631</td>\n",
       "      <td>-0.967954</td>\n",
       "      <td>-1.128704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.439141</td>\n",
       "      <td>0.092116</td>\n",
       "      <td>-0.777908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.390532</td>\n",
       "      <td>-1.086842</td>\n",
       "      <td>-7.193565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   indep_var1  indep_var2   dep_var\n",
       "0   -0.022755   -1.955844 -5.491880\n",
       "1    1.115004    0.179297  3.795530\n",
       "2   -0.295631   -0.967954 -1.128704\n",
       "3    1.439141    0.092116 -0.777908\n",
       "4   -0.390532   -1.086842 -7.193565"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indep_var = np.random.normal(size = (100,2))\n",
    "data = pd.DataFrame(data = indep_var, columns = ['indep_var1','indep_var2'])\n",
    "data['dep_var'] = 2*data.indep_var1 + 3*data.indep_var2+np.random.normal(loc = 0, scale = 2, size=(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Extend `Model` class (access data with `self.data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MSE_Regression(op.Model):\n",
    "    def predict(self, beta1, beta2):\n",
    "        return self.data['indep_var1']*beta1 + self.data['indep_var2']*beta2\n",
    "    \n",
    "    def loss(self, beta1, beta2):\n",
    "        prediction = self.predict(beta1, beta2)\n",
    "        return np.sum((prediction-self.data['dep_var'])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Instantiate the `Model` object with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = MSE_Regression(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Call `minimize_over_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.26279033, 2.82022594])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = op.minimize_over_data(model, [10,10], 'Gradient Descend', 40, stochastic = True, lr = 1e-3)\n",
    "r.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Stochastic Gradient Descent through Parameter Space')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XuclGX9//HXZ/YEu5zkIAmyEgqkopiuB6QMU/OslVpimoeK/ObX+PqzzMwwKQ+ZeagsIQ9QKmWaGW4esALNUAMPC7rmlgq4rggoyxn28Pn9cd8rw7CH2dmduWd23s/Hgwc7c99z35+5T5+5ruu+r8vcHRERyV+xqAMQEZFoKRGIiOQ5JQIRkTynRCAikueUCERE8pwSgYhInlMiaIOZvWVmR2dgPbeb2ffTvZ5UxW8HM7vCzO6IOqaexMxmmdmPoo4DwMx+YGb3RB2HZF7OJQIz+4SZ/dPM6s3sfTN7xswODqedZ2b/iDrGtrQWn7tf6O4/THF5Z5rZc2a20czeC//+hplZ90S8I3e/1t2/2tXlmNlIM3MzK2xnnh+YWYOZrQ//vW5mvzCz3bq6/nQJv9Ne7UzP6uOzs8Lvu9HMNphZrZndZGYFUcfVETObb2ZdPo4TlrmvmT1hZh+Y2VozW2xmJ3TnOtIppxKBmfUDHgF+DgwEhgNXA1ujjCsKZnYpcCvwE+AjwFDgQmAiUNzGZ7L+JE3we3fvS7CvP0fwPRdnczJIp/YSZ4TGu3sf4CjgLOBrnV1Aln6vNrVxHs0F5hGch7sC3wTWZTKuLnH3nPkHVABr25i2N7AFaAI2tMwH9Ad+A6wClgFXArG4z30NqAbWA68CB4bvvwV8C6gC6oHfA73CabsQJKRVwAfh37vHLfM84I1wmW8CX2onvlnAj+I+eyrwEsFB9F/guFa+a39gI3BaB9trFvAr4C/h/EcDJwIvhstfAfwg4TPnhNtpDfC9cDscHU77AXBP3LyHAf8E1gIvA5Pips0Hfgg8E26HJ4DB4bTlgIfbYQMwoZXYd1hX+F5BuJ4b4947Kdxea8NY9o+b9h2gNlz/v4Gj4pZzRbh91wOLgRHhtI8RnNDvh5/5QsL2vA2oDD/3HLBnOO2p8DttDL/TF5M8PttcZjjdgYuAGuDN8L3DgX8RHJf/Ag6Pm//D/dXGPvty3P79fiv7936C82U98ApQ0c7x5cBeca//APwi/PvyuO37KvC5hPPjGeDmcDv/CNgT+FsY12rgXmBAwvf6NsH5uBG4k+Ci+2i4jieBXTo6NoFrwn2wJdwPLfF2tN93OI8StsPgcFsMaGM7TQLeJjjmVoff5Utx0zs6Jz8R911WAOeF75cANxKcTyuB24HeKV1bu3pxzuQ/oF94oMwGjo/f8XEH2D8S3vsN8DDQFxgJvA58JZx2BsGF4mDAgL2APeIOvOeBYQS/SKuBC8Npg4DTgNJwuX8A/hROKwt36Njw9W7Avu3EN4swEQCHEJzcxxCU1oYDH2tlOxwHNAKFHWyvWeHyJobL6xUelPuFr/cPD6DPhvPvQ3ByHBEeZDeF69kpEYSxrQFOCJd1TPh6SDh9PsGFYAzQO3x9fThtJMGJ02b8tJIIwvenA8+Ffx8IvAccSnBxPzfcbyXAWIKTZljcOlsu2t8GloTzGDA+3Kdl4WfOBwrD5a+O23+zCC4Uh4TT7wV+FxfbDhfGVmJva/93tMx5BMdg7/D/DwgSdiEwOXw9KO64bTURxO3fTxCUGm8EGhL275ZwnxYA1wHPtvN9Pvy+4bLfZcdza1h4bHyR4AK6W9x2aAQuDr9Db4Jz75hw3w0hSKy3xK3rLeBZgov/8HC/vwB8PPzM34CrOnFsfjVu2cns9x3Oo4TtYASJ+hHgs8DQhOmTwu97Uxjrp8LtMTZuelvnZDlBopsMFBEcpweE024B/kxwTPQlKJVcl9K1Nd0X7+7+R/DLahZBhm0MN8TQ1k608GDeCuwT997Xgfnh348DU9tYz1vA2XGvbwBub2PeA4AP4g6qtQSJonfCfDvEF3eQtSSCGcDNSWyDs4F3E95r+cWwGTgibtm/6WBZt7SsE5jGjhehMmAbrSeC7wC/TVjW48C5cSfblXHTvgE8Fv49ktQTwYVATfj3r4AfJkz/N8GJthfBxeJooKiVeU5tZdlfBJ5OeG8G2y8ws4A74qadALwW9zrVRNDRMj8d9/oc4PmEZSxk+6/Et2g7EUwD5sRNK21l/z4ZN30fYHM738cJfvR8QJD0f0RcaTth3pdatnm4HZZ3cFx+Fngx4XyM/xX9IPCruNcXs/3HWDLHZnwiSGa/d3Qe7Q78ItwOzQSJbHQ4bRLBtaosbv77ge8ncU5+F3iolXmMIJnElx4nEJYaO/svp9oIANy92t3Pc/fdgXEEvzpuaWP2wQS/fJbFvbeM4BcDwAiCHdeWd+P+3gT0ATCzUjObYWbLzGwdwU4fYGYF7r6R4MC6EKgzs0oz+1iSX6+jeFqsAQbH1626++HuPiCcFr9fV8R/0MwONbO/m9kqM6sP4xwcTh4WP3/4Xda0EcMewBlhw9haM1tL8Eszvv6+1e3XRcMJfkG3xHBpQgwjCEoB/wH+j+Di9p6Z/c7MhoWfa2s77wEcmrC8LxG0TaTzO3W0zPh9OIwdj2fY8ZhuT+L+3cTO+zcxll4d1OEf6O67uPue7n6luzcDmNmXzeyluO04ju3HWeJ3wsx2DfdRbXhO3ZMwPwS/lFtsbuV1y3ZL5tiMl8x+X9H6RwPu/ra7/6+77xkubyNBbUSLD8LzqcUygv3R0TnZ1rE6hCCRL46L+bHw/U7LuUQQz91fI8jW41reSphlNUHRd4+498oJqoMg2Ll7prDqSwmqFQ51934EVSkQZGnc/XF3P4bgwHsN+HUb8SVKNp6FBCWdU5OYN3Gd9xGUoka4e3+CesWWu4zqCA48IEh4BEXRtmL9rbsPiPtX5u7XpxBTUswsBpwMPB0XwzUJMZS6+xwAd7/P3T9BsP8d+HHc51rbziuABQnL6+Pu/5NKvK1I6XsnfO4ddjyeYcdjeiPBBaJF/MWsjuCXKwBm1pu292/KzGwPgmP+fwmqrAYAS9l+nMHO2+K68L39w3Pq7IT5O6OjYzNx3cns96T3nbuvIGj3GRf39i5mVhb3upxgX0L752Rbx+pqguS3b1zM/T1ouO+0nEoEZvYxM7vUzHYPX48gqDt7NpxlJbC7mRUDuHsTQRHsGjPrGx6g/4/g1wbAHcC3zOwgC+wVztORvgQ7Ya2ZDQSuiotxqJmdEu70rQR1sk2txdeKO4HzzewoM4uZ2fDWShPuvpbgbqlfmtnpZtYnnP8AguqcjmJ/3923mNkhBHd6tHgAOMmCW3SLCerj2zpG7gFONrNjzazAzHqZ2aSWfdOBVQTF51FJzIuZFZnZ3sAcggvbTeGkXwMXhr+ozMzKzOzEcF+PNbNPm1kJQb33ZrbvhzuAH5rZ6PBz+5vZIII63jFmdk64ziIzOzhcdzJWdvCdOtr/yfhLGONZZlZoZl8kqMJ5JJz+EnBmGHsFcHrcZx8g2GeHhzFcTeoX2/aUEVw4VwGY2fnseFFsTV/CRnQzG07QjpOqjo7NxP3Upf1uZruY2dXh9SNmZoOBC9h+XWpxtZkVm9knCW5y+EP4fnvn5L3A0Wb2hXB/DzKzA8KS16+Bm81s1zCO4WZ2bNJbKU5OJQKCRpNDgefMbCPBhl5K8AsdggajV4B3zWx1+N7FBL+S3gD+QZB97wJw9z8Q3EVwX7jsPxE0vHTkFoIGrtVhDI/FTYuF8bxDUIXxKYL68bbi+5C7P0/QYHUzQePUAnb+9dcy7w0ESe0ygrrwlQT1mt8haC9oyzeA6Wa2nqDO+P64Zb5CcIfKfQS/Hj8gaItpbf0rCEokVxCc8CsITt4Oj6mwSuIa4JmwWHtYG7N+0cw2ELR9/JmgGuMgd38nXM4igru+fhHG+h+C+mcIGuWuJ9hH7xLc0ndFOO2m8Hs/QVDHfSdBe8564DPAmQT7712CUkRJR98p9ANgdvidvtDK9Hb3fzLcfQ3BReRSgu1xGXCSu7cs7/sEvyA/ILjQ3xf32VcIzoffEezf9QTHTrfefu3urwI/JSi5riRoCH2mg49dTdBIW09wB9Ufu7D+jo7NW4HTLbjn/2fdsN+3EbR7PUlwPC0l2Kbnxc3zLsE+eYfg4n5hWKMB7Z+TywnajS4luJ68RHBzAwTn+n+AZy2oTnuSoKai0yxsZBCRPGNmfQiS7Gh3fzPqeHoqM5tE0GCfTGk5ErlWIhCRLjCzky242aGM4PbRJQR35EgeUyIQyS+nElRPvAOMBs50VQvkPVUNiYjkOZUIRETyXE509jR48GAfOXJk1GGIiOSUxYsXr3b3Dh8yy4lEMHLkSBYtWhR1GCIiOcXMEp9Cb5WqhkRE8pwSgYhInlMiEBHJc0oEIiJ5TolARCTPpS0RmNldFgyovjTuvYFmNs/MasL/d0nX+kVEJDnpLBHMIhhSMd7lwF/dfTTw1/C1iIjEqa6r5+Z5r/OtP7zMzfNep7quPq3rS9tzBO7+lJmNTHj7VIJh2yAYd3g+QVeqIiICVFbV8pPH/0395kYKzNilrIiqt9dy2XFj2Xu3/mlZZ6YfKBvq7nUA7l7XMqCCiEg+m7Gghtvnv0n9lgaaw+7fCoCS4hhb6pvYvLWJ3y5cxrWf3z8t68/aJ4vNbAowBaC8vDziaERE0mPGghque/T1nd5vAhoam4nFjE3bGnlxxdq0xZDpu4ZWmtluAOH/77U1o7vPdPcKd68YMiSl8ZhFRLLezfN2TgItmpqhIGZsamjG0jKqaCDTieDPwLnh3+cCD2d4/SIiWaG6rp7v/bGKLY1tz+NAc7PT7M7HR6SnfQDSWDVkZnMIGoYHm9nbBAO8Xw/cb2ZfAZYDZ6Rr/SIi2WjGghrufPot3t/UgNH+eDAONDsM7lPC2RNaHb68W6TzrqHJbUw6Kl3rFBHJNpVVtXzngSo2bGve4f1Cg4YOxgXrXWgM7d+bbx87Jm13DEEWNxaLiOS6yqpaps55icZWLviNDgZtlgmKYzDlU3tx3LihaU0CoEQgItLtquvqeWzpSn41v6bVJNAiZtDUyvSy4hg3nL4/J+4/PH1BxlEiEBHpBi0X/1fr6lmxZjNbGhvZ1tT+Z4zwjh2DophRWGCMHNyHb0walbEkAEoEIiJdVl1Xz8yn3qR/7yLqNzWAwbI1mzv8XLMHVUOHf3QgV568T9qrgNqiRCAi0knxTwID9C4yKvYYSPnAUtZvbaRfr8IO7gcKlBTFmHzI7kw7eb/0BtwBJQIRkTZU19Vzz8Jl/Omld9jYTj3Pxm3OMzVrKC0upF+vIrY0NLXbENy3EC741OiMNAQnQ4lARCRByy/+tZsbkvplD9AIvFxbz5Fjh7B42VoGlBbywabGD58HdoI2gVPGD+XWyRVpiTtVSgQiInFmLKjh5if/w9aG5qSTQIv1mxsZWFbC2KF9AOhT0sC79VtpanbKSgr4QsXwyKuBWqNEICISZ/bC5bg7MdvemJuMmEHf3oXU1W9h5OA+XDhpz6yo9kmGEoGI5LWWdoAXV9SzuaGRd9ZuSWk5fYpifO+Ej2X0ts/uokQgInmpsqqWm+fV8MbqjR+OAZCqAb1iXPP5zD0A1t2UCEQk78xYUMNPH3+dhO5/klYADOlXQvmgMs6dUJ6zCaCFEoGI5JXKqlp+/NjrnSoF9CqM8dBFh+dMnX9nKRGISN6orqvn2srqpJJA35ICSosLaGyGPXft02OTAGR+YBoRkcg8tnQlqzduS2rewpixuaGZwhicO6FnD5erEoGI9GgtncHVrt3MK+/U09Rad59xDCgphGZgxMDSjHcAFwUlAhHpsarr6vn+n5ay4v3NbGlsorGpmWYPqkJaayce3r+EK07cu8df+BMpEYhIjzJ97hLue+5ttjRuv9SXFcXo17uIzcBmC5JBUdwIYQXAuRPLs/Kp30xQIhCRHmP63CXc9czynd7f2NBMUWEjZSVFuMPmbU30Ky2ioamZ3XfJj+qf9kSSCMxsKvA1guq4X7v7LVHEISI9yz0Ld04CLTZtbaasBIoLDayQRVcek8HIslvG7xoys3EESeAQYDxwkpmNznQcItLztPeAWJNDY5OzpcEp36V35oLKAVHcPro38Ky7b3L3RmAB8LkI4hCRPNLs/mEvoBdOGhV1OFklikSwFDjCzAaZWSlwAjAicSYzm2Jmi8xs0apVqzIepIjkHmtnWkmhMWrXPnz/pPy7K6gjGW8jcPdqM/sxMA/YALxMMKZD4nwzgZkAFRUVXewSSkR6ohkLarjz6bdYs6mB5mZvs8toA04aP5wbzxifyfByRiRPFrv7ne5+oLsfAbwP1EQRh4jkrpYBZFZv3EZTO0kAoKjAGD5A7QJtiequoV3d/T0zKwc+D0yIIg4RyV0tA8i4B7/4LRxIJl5hDHAoLoxx3LihUYSZE6J6juBBMxsENAAXufsHEcUhIjnq/Q1b2da4vSTgCUnAgMJYjKIC45OjB/foTuO6KpJE4O6fjGK9ItIzVFbVsrWx7eqgmEFJYYwxQ/syuE8JFx+lO9TboyeLRSSnTJ+7hFn/XN5uw3Cvwhj7De/PYXsO5rhxQ1Ua6IASgYjkjBkLarj7mbaTAED/3kVc87l9dYtoJygRiEjWa60juUS9CmP0Li7gyxNGKgl0khKBiGS1qXMW8fDLKzucr7nZ6d+7UHcHpUCJQESyUnVdPfcsXJZUEgAY1KeYbx87Vu0BKVAiEJGsM33uEuY8335VULyy4hh3nX+wkkCKlAhEJCucdtvTLF6xrtOfKyk0bjh9fyWBLlAiEJHIpZoESotiTD16LzUOd5ESgYhkXPyA8iUFllISGNqniFlfOVQlgW6gRCAiGVVdV8/Mp96kf+8iigpgbtU7nfp8cYExetc+3PiF8UoC3USJQETSbsaCGm6f/yb1WxpodigthE+NHcr7mxpYv6UpqWX0L4kxbGAZHx8xgHMm7KEk0I2UCESkW1VW1XLzvBreWL1xp95AW2xqhHmvrmRI3+J2nxIGKDT48uHlTDt5v26PVQJKBCLSLWYsqOG2v/+XdUn+wm9y2LC1CYM2k8HEUQO58uR99Os/zZQIRKTLWgaJ2dKQ3H3/EFz8Hejbq6DV5HHq+KHcOrmi+4KUNikRiEhKWp78XfjGGt5YvanTnzegV1EBx+4zlH8tW8MbqzbT1Oz0KjQmHzpCVUEZpEQgIp02Y0ENN82rYWtj6sOJ9y2OcfUp++gZgCygRCAiSausqmXaw6+wZmNDl5YzoFeMaz6/v5JAllAiEJGkVFbVctkDVWzclnw7QLySGIzfYyDnTihXAsgyUQ1efwnwVYK2oiXA+e6+JYpYRCQ5v5z/RqeSQL+SAn5/4QTd8ZMDMp4IzGw48E1gH3ffbGb3A2cCszIdi4i0r7KqltkLl7Ny3RZWvJ9cg3BpUYy+vQqZpts+c0ZUVUOFQG8zawBKgc49Yy4iaVVdV8/P/1rD/NdXUxQzigpo8+GweAXAR4f04RuTRqn6J4dkPBG4e62Z3QgsBzYDT7j7E4nzmdkUYApAeXl5ZoMUyWPVdfWc8+tnWb2pcYf3Cw3aukmoKAbfOnYMX//U6AxEKN0tlukVmtkuwKnAR4FhQJmZnZ04n7vPdPcKd68YMmRIpsMUyVvn3blzEoAgCRQlXDGKYnDBxHJqrj1RSSCHRVE1dDTwpruvAjCzPwKHA/dEEIuIxKmuq2flhp2TQIuykkIampoZ3LcXnz1gOMeNG6p2gB4gikSwHDjMzEoJqoaOAhZFEIeIJPjtwmXtTm92OHjkQMaP2IVLjhmToagk3aJoI3jOzB4AXgAagReBmZmOQ0QCiV1Et+fA8gHsUlbCceOGZiY4yYhI7hpy96uAq6JYt4hsN2NBDT99ooZtTR3fElRgMH7ELqoO6oH0ZLFIHpu9cDmNzY4RdALX3uNiZx48QtVBPZQSgUgeq9+8vTqorTJBSy+hZ0/YI1NhSYYpEYjkqelzl7Bxa/uDyBQYlBYX8MnRg1Ud1IMpEYjkoelzlzDrn8vbnaekAMoHlVE+sIyLj9IzAj2ZEoFIHpk+dwn3Pfc2Wxrb7zyuMGZ8cswQ9h3WX43DeUCJQCRPTJ+7hLueab8UANC/dyFjP9KPO849OANRSTbIeBcTIpJ51XX1SSUBAHc4d4L698onKhGI9HBT5yyicsnKpOe/6Ej1HJpvlAhEeqDqunrOnPFP6rckP5BMzGDf3fqp87g8pEQg0gNU19Xz2NKV1K7dzKatDfxlafIlAAjuEBpQWsyFk0alKULJZkoEIjmuuq6emU+9Sf/eRRQVwJPV73V6GXsN7afBZPKYEoFIDqmsqmXaw6+wZmPDTtPKio2P9OudVL9BENwp8tEhpfzirAN1e2ieUyIQyQEzFtTws7/+p93B4zduc/67OrlxhXfvV8xpB++hZwQEUCIQyXqd6SE0GRdMLGfayft1y7KkZ1AiEMlCnRkjIJFZ8CxAotJCePVHJ3ZPgNKjKBGIZJmpcxbx8Mudu+sn3pCyYgaUFfLGqs00NTu9Co3Jh45QKUDapEQgkkW6mgT6Fce46pR9dPePdIq6mBDJEjMW1PDnLpUECrnu9P2VBKTTMl4iMLOxwO/j3hoFTHP3WzIdi0g2qKyqZfbC5Sxe9kGbg8O0ZtTgUkYN6aMeQqXLohi8/t/AAQBmVgDUAg9lOg6RqE2ds4hHqlbSmZuBWp4Annayqn+k+0TdRnAU8F93XxZxHCIZlUpbQAF6AljSI+pEcCYwJ+IYRDKuM0mgEPj28WPUGZykTWSJwMyKgVOA77YxfQowBaC8XH2jS/7pXRTjwBEDuPLkfVT/L2kVZYngeOAFd2/1p5G7zwRmAlRUVHTPI5UiEZqxoIbb/v5f1m9pf8D4PsUFHDFmML88uyJDkUm+izIRTEbVQpInZiyo4cbHa2hI4jHhfYb102DxklGRPEdgZqXAMcAfo1i/SKbNXrg8qSRQHDOuPnVfVQVJRkVSInD3TcCgKNYtkmnVdfW8u3ZLUvNeeuxoJQHJuHZLBGbWz8yuM7PfmtlZCdN+md7QRHJfdV09X737eToaMLLA4IARGiZSotFRieBuoAZ4ELjAzE4DznL3rcBh6Q5OJJedd+dC5te83+F8Q/sWMW74Lnzr2DEZiEpkZx0lgj3d/bTw7z+Z2feAv5nZKWmOSySnnXbb0yxesa7D+WIGZx4yUl1ESKQ6SgQlZhZz92YAd7/GzN4GngL6pD06kRw0fe6SpJJAccwY+5G+XHKMSgISrY4SwVzg08CTLW+4+2wzWwn8PJ2BieSaCdc+Tt26xqTn79OrgAsnjUpjRCLJaTcRuPtlbbz/GKBWLclr1XX1PLZ0Ja/W1TPv1fc69dmSAvjhZ8epzyDJCknfPmpmJwL7Ar1a3nP36ekISiSbzVhQw51Pv8X7mxooKjCamjq6J2hHu/YpZPZXDlObgGSNpBKBmd0OlAJHAncApwPPpzEukazQMlbAa3XrWLdl52qfxk4OKDx6SCk/O+tAJQHJKsmWCA539/3NrMrdrzazn6KngqWHq6yqZfrcV/lgYwPbOjuCfIJehXDJMepBVLJTsolgc/j/JjMbBqwBPpqekESiNX3uEu5ZuKLLF/8WF0ws18DxktWSTQSPmNkA4CfAC4ATVBGJ9CjJ3v+frEenfkLVQJL1kk0EN4RPEz9oZo8QNBgn13mKSI5I9v7/tsQIfiH1KjQmHzpCpQDJGckmgoXAgQBhQthqZi+0vCeS66rr6vntwhUpfbakAG764gG6FVRyVruJwMw+AgwHepvZxwELJ/UjuItIJGfNWFDD7fPfZO3mBlJtDRg1uJRLPzNGSUByWkclgmOB84DdgZvi3l8HXJGmmETSbsaCGn7y2Os0djID9Co0LjlmtO7+kR6loyeLZwOzzew0d38wQzGJpN2vn36zU0mgKAZjPtKPb0wapV//0uMk20bwjJndCQxz9+PNbB9ggrvfmcbYRLpdZVUt0x95ldUbGpKav6w4xg2n76+Lv/RoyQ5VeTfwODAsfP068H9piUgkTSqravnOg0tYuW5bUvPvO6yfkoDkhWRLBIPd/X4z+y6AuzeaWVMa4xLpdrMXLmfD1uQO2+8er6eAJX8kWyLYaGaDCG6TxswOA+pTXamZDTCzB8zsNTOrNrMJqS5LJFkvL+t4tDCAQWVFSgKSV5ItEfw/4M/AKDN7BhhC0PFcqm4FHnP3082sGN2KKmmU7JCREIwYNv3UfdMckUh2STYRvAo8BGwC1gN/Imgn6DQz6wccQXBbKu6+DUiu0lakkzrTZURRzDhnwgi1CUjeSTYR/Ibg2YFrw9eTgd8CZ6SwzlHAKuBuMxsPLAamuvvG+JnMbAowBaC8vDyF1Ui+60yXEcMG9OJ7J3xMSUDykrl3fDO1mb3s7uM7ei+pFZpVAM8CE939OTO7FVjn7t9v6zMVFRW+aNGizq5K8lR1XT2n/vwfbEtyvJiRg3rzq7MPUudw0uOY2WJ3r+hovmRLBC+a2WHu/my48EOBZ1KM7W3gbXd/Lnz9AHB5issS+VB1XT33LFzGvc93rs+gbx87VklA8lpHfQ0tIbhTqAj4spktD1/vQdBu0Gnu/q6ZrTCzse7+b+CoVJcl+au6rp4L7n6euiSfCWiNAedPLFd1kOS9jkoEJ6VpvRcD94Z3DL0BnJ+m9UgP1Jm7gNpSFINvHatnBUSg476GlqVjpe7+EtBhvZVIi8qqWqY9/AprNibXNUR7Th0/lFsn6/ATaZFsG4FIZKbPXcJdzyzv8nJiQKVGDBPZSbJPFotEorKqlru7IQmAkoBIW1QikKw1Y0ENNz5ek/KgMRA8JHbCfruqKkikHUoEkpUOmv4oazYl+SBAnELgyxPLNV6wSCcoEUjWmXTDk51KAn2K4MdnaMxgkVQpEUjWmLGghp/99T9sTPKR4H7F8Pv/Ub2/SFcpEUhWaGkPaGjuuEWguADOPkzVPyLdRYlAssLN816nIYmCgAaMEel+un1UIlVZVcvoKyrZ0tjxvAZKAiJpoBKBRKayqpapv3uJxiRYDzbHAAANkUlEQVTbhS8/fkx6AxLJU0oEEolJNzzJW+9vTXr+CyaWqzQgkiZKBJJR1XX1fP62f7A5iaogCDqH+0LFCDUMi6SREoGkVWd/+ScaP2IAZ0/YoxsjEpFESgTS7WYsqOHOp9/ivQ1dG4p69JBSfvjZcXpOQCTNlAikW02ds4iHX17Z5eVMGj2QWV+Z0A0RiUhHlAik21RW1XY5CWjAGJHMUyKQbjFjQQ3XP/p6l5ax99BSHr3kyG6KSESSpUQgXTb2ikq2dr6j0A8Vx+DsCeoyQiQqkSQCM3sLWA80AY3urs7ic1Cq7QEGnKLhIkWyRpQlgiPdfXWE65cuSCUJqAFYJDupakhS0pkkoLp/kewWVadzDjxhZovNbEprM5jZFDNbZGaLVq1aleHwpD37X1WZ1HyFBF1DKAmIZLeoSgQT3f0dM9sVmGdmr7n7U/EzuPtMYCZARUVFV4atlW7S2e4h/nP9iekNSES6RSSJwN3fCf9/z8weAg4Bnmr/UxKl8+5cyPya95Oe/7azDkhjNCLSnTKeCMysDIi5+/rw788A0zMdh7TvtNueZvGKdZ3+XGEMbj1T4weL5JIoSgRDgYfMrGX997n7YxHEIQlmLKjhF3/7L+u3NqW8jG8fO0ZJQCTHZDwRuPsbwPhMr1faN2NBDdd18cngSaMHqmsIkRykoSqF6rr6bkkCekZAJDfpOYI8d/zNf6d65aYuLUMDyovkNiWCPFVZVcvUOS/R2IUbcweVxlg87fjuC0pEIqFEkIcqq2r5zoNLOp0EYsCZh4zgnAl7aLAYkR5EiSAPzV64nA2duDPIgOP2Hco3jx6tBCDSAykR5Jmpcxbx/JvJPRimPoJE8oMSQR4ZdXklyQ4boAZgkfyhRNCDpXpH0N5DS5UERPKIEkEPlGr3EAB7DOyl6iCRPKNE0MNMuPZx6tYl2T1ogiKD28/RqGEi+UaJoAfZ58pKNqWWAwC4ZfIBuitIJA8pEfQAne0iOlFpITx40SeUBETylBJBjutqEtAtoiKiRJDDupIEimLGCfvtyq2T1SYgku+UCHJUZ54JgGBH33qWBowRkZ0pEeSYGQtquP7R1+lMN0F6OExE2qNEkAOmz13CXc8sT+mzSgIi0hElgiw3dc4iHn55Zac/VwBcpiQgIkmILBGYWQGwCKh195OiiiObpfpwWHEMXr/2xDREJCI9UZQlgqlANdAvwhiy1l6XV5Lqs2EPX/yJbo1FRHq2SMYsNrPdgROBO6JYf7YblWISKAQenaoHw0Skc6IqEdwCXAb0jWj9WWvk5ZUpfU6Dx4tIqjKeCMzsJOA9d19sZpPamW8KMAWgvLw8Q9FlXld6CgW4YGI5007erxsjEpF8E0WJYCJwipmdAPQC+pnZPe5+dvxM7j4TmAlQUVHRhSHWs1eq4wX0K4Gqq9UYLCLdI+NtBO7+XXff3d1HAmcCf0tMAj1ddV0946b9JaUkMGn0QCUBEelWeo4gw6rr6jn+1n+k9Nnd+hWqHUBEul2kicDd5wPzo4wh01JNAv1KYOEVx3ZzNCIiKhGkXWVVLd/6w8tsbki9meOgEf148KJPdmNUIiLbKRGkUWVVLRfd91LKny8ATho/VF1Fi0haKRGkSapjBah7CBHJNCWCNEh17OChfYuZdcEh3R+QiEg7lAi62Wm3PZ1SEpg4aiBXnryPuocQkYxTIugmB01/lDWbOjNm2HZ7Dy3l3im6LVREoqFE0EVdHTz+VDUGi0jElAi6oLPjBsdTAhCRbKFEkIJUG4MBbtMA8iKSZSIZjyBXVdfVM/JyJQER6VmUCJJUWVWbcvcQEDwdrCQgItlIVUNJSHWwmBYaNEZEspkSQTtmLKjhukdfT+mz6h9IRHKFEkErKqtqueR3L7EtxVuC1BYgIrlEiSBBqqOGtXjrevUTJCK5RYkgTleeC1A7gIjkKiUCuj6AvEoBIpLL8j4RdPWOICUBEcl1eZsIujJ2MMB3jx/D1z81uhsjEhGJRsYTgZn1Ap4CSsL1P+DuV2Uyhq50EVFaCK/+SKUAEek5oigRbAU+7e4bzKwI+IeZPeruz6Z7xV29I2jvoaU8esmR3RiRiEj0Mp4I3N2BDeHLovBf6iO7J6krbQH9SqDqapUCRKRniqSNwMwKgMXAXsBt7v5cK/NMAaYAlJeXp7yuqXMW8fDLK1P+/G79Cll4xbEpf15EJNtFkgjcvQk4wMwGAA+Z2Th3X5owz0xgJkBFRUVKJYau3hGkJ4RFJB9EeteQu681s/nAccDSDmbvlK4kgUGlMRZPO74boxERyV4Z74bazIaEJQHMrDdwNPBad65j+twlKX/2u8ePURIQkbwSRYlgN2B22E4QA+5390e6cwX3L6rt9GfUICwi+SqKu4aqgI+ncx1bGjrXY5DaAkQkn/XIJ4t7FcXYsLUpqXnVRYSI5LseOVTlFyqS+3WvJCAi0kMTwbST92PvoaVtTr/trAOUBEREQj0yEQA8esmRXDCxnD4lBRTGjD4lBVwwsZy3rj9R7QEiInEs6PEhu1VUVPiiRYuiDkNEJKeY2WJ3r+hovh5bIhARkeQoEYiI5DklAhGRPKdEICKS55QIRETyXE7cNWRmq4BlrUwaDKzOcDjJUmypUWypy+b4FFtquhrbHu4+pKOZciIRtMXMFiVza1QUFFtqFFvqsjk+xZaaTMWmqiERkTynRCAikudyPRHMjDqAdii21Ci21GVzfIotNRmJLafbCEREpOtyvUQgIiJdpEQgIpLnci4RmNkIM/u7mVWb2StmNjXqmFqYWS8ze97MXg5juzrqmBKZWYGZvWhm3TpOdHcws7fMbImZvWRmWdXdrJkNMLMHzOy18NibEHVMAGY2NtxeLf/Wmdn/RR1XCzO7JDwXlprZHDPrFXVMLcxsahjXK9mwzczsLjN7z8yWxr030MzmmVlN+P8u6Vh3ziUCoBG41N33Bg4DLjKzfSKOqcVW4NPuPh44ADjOzA6LOKZEU4HqqINox5HufkAW3td9K/CYu38MGE+WbEN3/3e4vQ4ADgI2AQ9FHBYAZjYc+CZQ4e7jgALgzGijCpjZOOBrwCEE+/MkMxsdbVTMAo5LeO9y4K/uPhr4a/i62+VcInD3Ond/Ifx7PcEJmRUjzXhgQ/iyKPyXNa3xZrY7cCJwR9Sx5BIz6wccAdwJ4O7b3H1ttFG16ijgv+7e2lP4USkEeptZIVAKvBNxPC32Bp51903u3ggsAD4XZUDu/hTwfsLbpwKzw79nA59Nx7pzLhHEM7ORwMeB56KNZLuw6uUl4D1gnrtnTWzALcBlQHPUgbTBgSfMbLGZTYk6mDijgFXA3WG12h1mVhZ1UK04E5gTdRAt3L0WuBFYDtQB9e7+RLRRfWgpcISZDTKzUuAEYETEMbVmqLvXQfAjGNg1HSvJ2URgZn2AB4H/c/d1UcfTwt2bwmL67sAhYRE0cmZ2EvCeuy+OOpZ2THT3A4HjCar8jog6oFAhcCDwK3f/OLCRNBXRU2VmxcApwB+ijqVFWJ99KvBRYBhQZmZnRxtVwN2rgR8D84DHgJcJqp3zUk4mAjMrIkgC97r7H6OOpzVh1cF8dq7zi8pE4BQzewv4HfBpM7sn2pB25O7vhP+/R1DPfUi0EX3obeDtuNLdAwSJIZscD7zg7iujDiTO0cCb7r7K3RuAPwKHRxzTh9z9Tnc/0N2PIKiSqYk6plasNLPdAML/30vHSnIuEZiZEdTVVrv7TVHHE8/MhpjZgPDv3gQnwmvRRhVw9++6++7uPpKgCuFv7p4Vv84AzKzMzPq2/A18hqD4Hjl3fxdYYWZjw7eOAl6NMKTWTCaLqoVCy4HDzKw0PG+PIksa2QHMbNfw/3Lg82Tf9gP4M3Bu+Pe5wMPpWElhOhaaZhOBc4AlYV08wBXu/pcIY2qxGzDbzAoIkuz97p51t2lmqaHAQ8H1gkLgPnd/LNqQdnAxcG9YBfMGcH7E8XworOM+Bvh61LHEc/fnzOwB4AWCapcXya7uHB40s0FAA3CRu38QZTBmNgeYBAw2s7eBq4DrgfvN7CsEifWMtKxbXUyIiOS3nKsaEhGR7qVEICKS55QIRETynBKBiEieUyIQEclzSgQiITMbGd/zYxLzn2dmw5KY74ywh8tmM8u2zvRElAhEuuA8gq4TOrKU4IGlp9IajUiKlAhEdlRoZrPNrCocf6DUzA4yswVhZ3iPm9luZnY6UEHwkNlLZtbbzKaZ2b/CPu5nhk/T4u7V7v7vaL+WSNuUCER2NBaY6e77A+uAi4CfA6e7+0HAXcA17v4AsAj4UjgewGbgF+5+cNj3fm/gpGi+gkjn5GIXEyLptMLdnwn/vge4AhgHzAt/4BcQdKncmiPN7DKCfvcHAq8Ac9MbrkjXKRGI7Cixz5X1wCvu3u7QlOEQjL8kGI1rhZn9AMiaYRlF2qOqIZEdlceNRzwZeBYY0vKemRWZ2b7h9PVA3/Dvlov+6nCsjNMzFbBIVykRiOyoGjjXzKoIqnd+TnBR/7GZvQy8xPY+9WcBt4e94G4Ffg0sAf4E/KtlgWb2ubA3yQlApZk9nqHvIpIU9T4qIpLnVCIQEclzSgQiInlOiUBEJM8pEYiI5DklAhGRPKdEICKS55QIRETy3P8HugbYTX+PWH8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.array(r.val_rec)[:,0],np.array(r.val_rec)[:,1], alpha = .5)\n",
    "plt.xlabel('beta1')\n",
    "plt.ylabel('beta2')\n",
    "plt.title('Stochastic Gradient Descent through Parameter Space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Future Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Matrix Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. The analysis of multivariate normal\n",
    "2. Elliptical contoured distributions\n",
    "3. Mixture models, such as expecation-maximization algorithm"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
