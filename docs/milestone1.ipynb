{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1: *AutoDiffAll*\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Background](#background)\n",
    "3. [User API](#API)\n",
    "4. [Software Organization](#SoftwareOrganization)\n",
    "5. [Implementation](#implementation)\n",
    "    - [Core Data Structure](#p1)\n",
    "    - [Major Class](#p2)\n",
    "    - [Method and Name Attributes in AutoDiff Class](#p3)\n",
    "    - [Other Functions](#p4)\n",
    "    - [External Dependences](#p5)\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "## Introduction \n",
    "\n",
    "It goes without saying that taking derivatives is an essential operation in numerical methods, optimization, and science. From a computational perspective, however, calculating a derivative can be a difficult.\n",
    "\n",
    "If one uses **finite differences** (i.e. $f'(x) \\approx (f(x+\\epsilon) - f(x))/\\epsilon)$), one needs to choose $\\epsilon$ appropriately. If $\\epsilon$ is too large, the approximation is poor. If $\\epsilon$ is too small, one introduces round-off errors.\n",
    "\n",
    "Alternatively, if one uses **symbolic differentiation** (i.e. an algorithm that produces the derivative as a symbolic function), the problem becomes computationally infeasible when you either have functions with many inputs or want to take high order derivatives. These two scenarios occur often in applications.\n",
    "\n",
    "**Automatic differentiation** overcomes these challenges by providing both quick and accurate derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"background\"></a>\n",
    "## Background\n",
    ">To do: Describe (briefly) the mathematical background and concepts as you see fit.  You **do not** need to\n",
    "give a treatise on automatic differentation or dual numbers.  Just give the essential ideas (e.g.\n",
    "the chain rule, the graph structure of calculations, elementary functions, etc).\n",
    "\n",
    "Automatic differentiation computes derivatives recursively using the chain rule. All functions are either an **\"elementary\" function**, for which we know the derivative, or a composition of elementary functions. To calculate the derivative of a composite function $f(g(x))$, we apply the chain rule as follows:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{df}{dg}\\frac{dg}{dx}\n",
    "$$\n",
    "\n",
    "The chain rule can be applied recursively, which we exploit in automatic differentiation. For example, if we have a complex composite function $f(g(h(x)))$, we can compute f'(x) by first calculating\n",
    "\n",
    "$$\n",
    "\\frac{dg}{dx} = \\frac{dg}{dh}\\frac{dh}{dx}\n",
    "$$\n",
    "\n",
    "and then plugging this derivative into \n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{df}{dg}\\frac{dg}{dx}\n",
    "$$\n",
    "\n",
    "This is in fact a simple example of **forward-mode** automatic differentiation. In general, to conduct forward mode automatic differntation, we represent our function to differentiate as a **computational graph**. The computational graph  captures the inputs and outputs of our elementary functions. In an example that can be found [here](http://www.columbia.edu/~ahd2125/post/2015/12/5/), we can represent $f(x,y)=\\cos(x)\\sin(y)+\\frac{x}{y}$ as \n",
    "\n",
    "![comp-graph](figs/comp_graph_background.png)\n",
    "\n",
    "By computing derivatives recursively using the chain rule from the inputs $x$ and $y$ to the output $f$, we can calculate the derivative over the entire graph.\n",
    "\n",
    "This project will implement only forward-mode automatic differentiation, but as an aside, **reverse-mode automatic differentiation** begins at the output(s) of the computational graph and calculates derivates using the chain rule by traversing the graph backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"API\"></a>\n",
    "## User API \n",
    "\n",
    "### Initial Setting\n",
    "The user would import the main `AutoDiff` class, as well as the `numpy` elementary functions that we have overridden.\n",
    "\n",
    "```python\n",
    "from AutoDiff import variable \n",
    "import AutoDiff.numpy as anp\n",
    "```\n",
    "\n",
    "### User Case\n",
    "User can write the function directly using the variable being declared as AD nodes and using ADF function like:\n",
    "\n",
    "```python\n",
    "x = variable(2) # set variable x to 2\n",
    "y = variable(3) # set variable y to 3\n",
    "f = anp.sin(x)+anp.exp(y)\n",
    "```\n",
    "Or write a lambda function and fit the AD nodes later like:\n",
    "\n",
    "```python\n",
    "f = lambda x, y: anp.sin(x)+anp.exp(y)\n",
    "result = f(x, y)\n",
    "```\n",
    "\n",
    "Our `AutoDiff` class can handle scalar or vector function and scalar or vector inputs.  \n",
    "\n",
    "More examples are as follows:\n",
    "\n",
    "#### Scalar Function\n",
    "- scalar values.\n",
    "    \n",
    "In the case for a scalar function, the implementation is simple. The user initializes an independent variable x at a given value. The user can then use their function on the variable instance just as they would previously on a given float/integer. They can then use the `der` function, which returns the derivative of the function with respect to the input variable.\n",
    "\n",
    "\n",
    "```python\n",
    "# unit variable case\n",
    ">>> x = variable(3)\n",
    ">>> f = x**2\n",
    ">>> print(f.val, f.der)\n",
    "9, 6 \n",
    "```\n",
    "\n",
    "```python\n",
    "# multi variable case\n",
    ">>> x = variable(3)\n",
    ">>> y = variable(2)\n",
    ">>> f = x**2+y**2\n",
    ">>> print(f.val)\n",
    ">>> print(f.der(x))\n",
    ">>> print(f.der(y))\n",
    ">>> print(f.grad())\n",
    "13\n",
    "6\n",
    "4\n",
    "[6, 4]\n",
    "```\n",
    "\n",
    "- vector values\n",
    "    \n",
    "For a scalar function of $d$ vectors with length $n$, the implementation is largely similar. The user has to initialize 2 separate varible instances for each independent variable. The rest of the implementation is exactly the same as before. However, we note that the `der` function would return a $n$ array of derivative with respect to all the independent variables at each value in the vector function. This is calculated element-wise.\n",
    "\n",
    "```python\n",
    ">>> x = variable([2.0, 5.0]) \n",
    ">>> y = variable([1.0, 2.0]) \n",
    ">>> f = x**2+y**2\n",
    ">>> print(f.val)\n",
    ">>> print(f.der(x))\n",
    ">>> print(f.der(y))\n",
    ">>> print(f.grad())\n",
    "[5, 29]\n",
    "[4, 10]\n",
    "[1, 4]\n",
    "[[4, 1], [10, 4]]\n",
    "\n",
    "```\n",
    "\n",
    "#### Vector Function\n",
    "Even if we have a vector function of vectors with an input of dimension n and and output of dimension m respectively, with multiple independent variables d, the implementation remains the same. However, when the user calls the vector function on the variable instances, he/she gets returned a vector of variable instances. As such, the user will have to call the `der` function separately on each AD instance.\n",
    "\n",
    "```python\n",
    ">>> x = variable([2.0, 5.0]) \n",
    ">>> y = variable([1.0, 2.0]) \n",
    ">>> f = [x**2+y**2, x+y]\n",
    ">>> print(f.val)\n",
    ">>> print(f.der(x))\n",
    ">>> print(f.der(y))\n",
    ">>> print(f.jacobian())\n",
    "[[5,3], [29,7]]\n",
    "[[4,1], [10,1]]\n",
    "[[2,1], [4,1]]\n",
    "[[[4, 2],[1,1]], [[10,5],[4,2]]] # These are jacobians for the first (x,y) pair, second (x,y) pair, and so on...\n",
    "\n",
    "```\n",
    "\n",
    "### Project extension: User defined functions\n",
    "\n",
    "The user can define their own function as follows. For example, imagine if a user wanted to implement the natural logarithm (although this is included in our numpy module anyways...)\n",
    "\n",
    "```python\n",
    ">>> log = user_function(np.log, lambda x: 1/x):\n",
    ">>> x = variable(1)\n",
    ">>> f = log(x)\n",
    ">>> print(f.val)\n",
    ">>> print(f.der)\n",
    "0\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"SoftwareOrganization\"></a>\n",
    "## Software Organization \n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "The directory structure will be as follows\n",
    "\n",
    "`\n",
    "AutoDiff\n",
    "|-README.md\n",
    "|-LICENSE\n",
    "|-setup.py\n",
    "|-requirements.txt\n",
    "|-AutoDiff\n",
    "  |-__init__.py\n",
    "  |-variables.py\n",
    "  |-numpy.py\n",
    "  |-user_func.py\n",
    "  |-derivatives.py\n",
    "|-docs\n",
    "  |-documentation.md\n",
    "|-tests\n",
    "  |-__init__.py\n",
    "  |-test_variables.py\n",
    "  |-test_numpy.py\n",
    "  |-test_user_func.py\n",
    "  |-test_derivatives.py\n",
    "`\n",
    "### Modules\n",
    "\n",
    "The `variables` module contains the functionality to define variables that are compatible with automatic differentiation. These variables will be passed to functions in `AutoDiff.numpy` or to functions specified by the user with `user_func`.\n",
    "\n",
    "The `derivatives` module stores our derivative functionality that will be accessed by the variables outputted by our functions.\n",
    "\n",
    "### Test Suite\n",
    "\n",
    "We will store our tests in the `tests` module and run them using `TravisCI`.\n",
    "\n",
    "### Distribution\n",
    "\n",
    "We will distribute our package on `PyPI`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"implementation\"></a>\n",
    "## Implementation \n",
    "<a id=\"p1\"></a>\n",
    "\n",
    "### Major data structure: Variables and the Computational Graph\n",
    "\n",
    "Our variables will be the nodes in our computational graph. The variables will keep track of the node's value and it's derivative.\n",
    "\n",
    "<a id=\"p2\"></a>\n",
    "### Classes\n",
    "\n",
    "The main class that we will implement is the `variable` class. All auto-differentiable functions will have inputs and outputs consisting of instances of the `variable` class. The `variable` class will be an extension on ordinary python numbers that will also store a derivative in addition to it's value.\n",
    "\n",
    "<a id=\"p3\"></a>\n",
    "### Method and Name Attributes in Variable Class\n",
    "* Name Attributes\n",
    "\n",
    "The `variable` class will have two main instance variables, the value of the variable instance, and the derivatives of the instance.\n",
    "\n",
    "`variable.val`: value of the variables. The shape is the same as the input variable. So if input is scalar, it will be scalar, while if input is vector, it is vector.\n",
    "\n",
    "`variable.der`: value of the derivatives. The shape is the same as the input variable. So if input is scalar, it will be scalar, while if input is vector, it is vector.\n",
    "\n",
    "* Methods\n",
    "\n",
    "In order to override the four basic operations of elementary arithmetic (addition, subtraction, multiplication, and division), we use dunder methods within our `variable` class. The dunder methods return new `variable` instances with the updated value and derivatives.\n",
    "\n",
    "We will also implement methods that will allow the user to access the derivatives of the variable.\n",
    "\n",
    "<a id=\"p4\"></a>\n",
    "### Other function \n",
    "\n",
    "* Define elementary differentiation function. \n",
    "\n",
    "In order to deal with the other elementary functions (exponential, logarithm, powers, roots, trigonometric functions, inverse trigonometric functions, hyperbolic functions, etc.), we will override the numpy elementary functions such that we can use it for our AutoDiff class. \n",
    "\n",
    ">For example, we will override the `np.sin` function such that if you use it on an `variable` instance `x` at a given value, it will return another `variable` instance with the value of $\\sin(x)$, and the calculated derivative of $\\dot{x}\\cos(x)$ at the given value. Similarly, we will override the `np.exp` function such that if you use it on an `variable` instance `x` at a given value, it will return another `variable` instance with the value of $\\exp(x)$, and the calculated derivative of $\\dot{x}\\exp(x)$ at the given value.\n",
    "\n",
    "To define an auto-differentiable function, the user will pass the expression for the value and derivative to the `user_function` method, which will return a function compatible with `variable` inputs.\n",
    "\n",
    "<a id=\"p5\"></a>\n",
    "### External dependencies \n",
    "\n",
    "In order to implement this, we will rely on the numpy and math external libraries, which will be specified as our external dependencies in our setup.py file.\n",
    "\n",
    "\n",
    "As such, after the user initializes the AutoDiff class on the indepndent variables, he/she will be able to use the usual elementary functions on the AutoDiff instance in order to calculate both the value of the function and the value of the derivative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
