{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1: *AutoDiffAll*\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Background](#background)\n",
    "3. [User API](#API)\n",
    "4. [Software Organization](#SoftwareOrganization)\n",
    "5. [Implementation](#implementation)\n",
    "    - [Core Data Structure](#p1)\n",
    "    - [Major Class](#p2)\n",
    "    - [Method and Name Attributes in AutoDiff Class](#p3)\n",
    "    - [Other Functions](#p4)\n",
    "    - [External Dependences](#p5)\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "## Introduction \n",
    ">Todo: Describe problem the software solves and why it's important to solve that problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"background\"></a>\n",
    "## Background\n",
    ">To do: Describe (briefly) the mathematical background and concepts as you see fit.  You **do not** need to\n",
    "give a treatise on automatic differentation or dual numbers.  Just give the essential ideas (e.g.\n",
    "the chain rule, the graph structure of calculations, elementary functions, etc).\n",
    "\n",
    "It goes without saying that taking derivatives is an essential operation in numerical methods, optimization, and science. From a computational perspective, however, calculating a derivative can be a difficult.\n",
    "\n",
    "If one uses **finite differences** (i.e. $f'(x) \\approx (f(x+\\epsilon) - f(x))/\\epsilon)$), one needs to choose $\\epsilon$ appropriately. If $\\epsilon$ is too large, the approximation is poor. If $\\epsilon$ is too small, one introduces round-off erros.\n",
    "\n",
    "Alternatively, if one uses **symbolic differentiation** (i.e. an algorithm that produces the derivative as a symbolic function), the problem becomes computationally infeasible when you either have functions with many inputs or want to take high order derivates. These two scenarios occur often in applications.\n",
    "\n",
    "**Automatic differentiation** overcomes these challenges by providing both quick and accurate derivatives. It does so by computing derivatives recursively using the chain rule. All functions are either an **\"elementary\" function**, for which we know the derivative, or a composition of elementary functions. To calculate the derivative of a composition $f(g(x))$, we apply the chain rule as follows:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{df}{dg}\\frac{dg}{dx}\n",
    "$$\n",
    "\n",
    "The chain rule can be applied recursively, which we exploit in automatic differentiation. For example, if we have a complex compositional function $f(g(h(x)))$, we can compute f'(x) by first calculating\n",
    "\n",
    "$$\n",
    "\\frac{dg}{dx} = \\frac{dg}{dh}\\frac{dh}{dx}\n",
    "$$\n",
    "\n",
    "and then plugging this derivative into \n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{df}{dg}\\frac{dg}{dx}\n",
    "$$\n",
    "\n",
    "This is in fact a simple example of **forward-mode** automatic differentiation. In general, to conduct forward mode automatic differntation, we represent our function to differentiate as a **computational graph**. The computational graph  captures the inputs and outputs of our elementary functions. In an example that can be found [here](http://www.columbia.edu/~ahd2125/post/2015/12/5/), we can represent $f(x,y)=\\cos(x)\\sin(y)+\\frac{x}{y}$ as \n",
    "\n",
    "![comp-graph](figs/comp_graph_background.png)\n",
    "\n",
    "By computing derivatives recursively using the chain rule from the inputs $x$ and $y$ to the output $f$, we can calculate the derivative over the entire graph.\n",
    "\n",
    "This project will implement only forward-mode automatic differentiation, but as an aside, **reverse-mode automatic differntiation** begins at the output(s) of the computational graph and calculates derivates using the chain rule by traversing the graph backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"API\"></a>\n",
    "## User API \n",
    "\n",
    "### Initial Setting\n",
    "The user would import the main `autodiff` class, as well as the `numpy` elementary functions that we have overridden. The usage of our elementary function stored in the AutoDiffFun\n",
    "\n",
    "```python\n",
    "from AutoDiff import AutoDiff as AD # Josh: can we come up with a better name? These are supposed to be the function inputs, right?\n",
    "from AutoDiff import AutoDiffFun as ADF\n",
    "import AutoDiff.numpy as np\n",
    "```\n",
    "\n",
    "### User Case\n",
    "User can write the function directly using the variable being declared as AD nodes and using ADF function like:\n",
    "\n",
    "```python\n",
    "x=AD(2,n_dim=1)\n",
    "f=ADF.sin(x)+ADF.exp(y) #Josh: What is y? Can we have the functions live in our overwritten version of numpy\n",
    "```\n",
    "Or write a lambda function and fit the AD nodes later like:\n",
    "\n",
    "```python\n",
    "f=lambda x: ADF.sin(x)+ADF.exp(y) #Josh: Again, what is y?\n",
    "result=f(AD(2,n_dim=1)\n",
    "```\n",
    "\n",
    "Our `AutoDiff` class can handle scalar or vector function in scalar or vector value, and can even handle when there are no explicit expression of the derivative for certain points (differentiate the non-differentiable function). \n",
    "\n",
    "More examples are as follows:\n",
    "\n",
    "#### Scalar Function\n",
    "- scalar values.\n",
    "    \n",
    "In the case for a scalar function, the implementation is simple. The user calls initializes AD instances on the independent variable x at a given value. The user can then use their function on the AD instance just as they would previously on a given float/integer. They can then use the `der` function that basically just returns the `der` instance attribute.\n",
    "\n",
    "\n",
    "```python\n",
    "# unit variable case\n",
    "x = AD(0, n_dim=1) \n",
    "test_fn1 = x - ADF.exp(-2*ADF.sin(4x))\n",
    "print(test_fn1.val, test_fn1.der)\n",
    "```\n",
    "\n",
    ">Output:\n",
    ">-1, -1\n",
    "\n",
    "```python\n",
    "# multi variable case\n",
    "x=AD(1.0, n_dim=2)\n",
    "y=AD(2.0, n_dim=3)\n",
    "test_fn2 = xy+(x+y)*6\n",
    "print(test_fn2.val)\n",
    "print(test_fn2.der)\n",
    "```\n",
    "\n",
    ">Output\n",
    ">20\n",
    ">[8, 7]\n",
    "\n",
    "- vector values\n",
    "    \n",
    "For a scalar function of vectors with length n with multiple independent variables d, the implementation is largely similar. The user has to initialize 2 separate AD instances for each independent variable. Moreover, the user has to indicate the number of independent variables d at initialization. The rest of the implementation is exactly the same as before. However, we note that the der function would return a n*d array of derivative with respect to all the independent variables at each value in the vector function.\n",
    "\n",
    "```python\n",
    "test_fn2 = lambda x, y: x*y + np.sin(x)\n",
    "x = AD([2.0, 5.0, 7.0], n_dim=2) \n",
    "y = AD([1.0, 2.0, 3.0], n_dim=2) \n",
    "ad_res = test_fn2(x, y) \n",
    "der(ad_res) # returns a 3x2 array of derivatives with respect to both x and y at each value\n",
    "```\n",
    "\n",
    "#### Vector Function\n",
    "Even if we have a vector function of vectors with lengths m and n respectively, with multiple independent variables d, the implementation remains the same. However, when the user calls the vector function on the AD instances, he/she gets returned a vector of AD instances. As such, the user will have to call the der function separately on each AD instance.\n",
    "\n",
    "```python\n",
    "test_fn4 = lambda x, y: (xy + np.sin(x), x+y+np.sin(xy))\n",
    "ad_res = test_fn4(x,y) \n",
    "der(ad_res[0]) \n",
    "der(ad_res[1])\n",
    "```\n",
    "\n",
    "- scalar value\n",
    "    \n",
    "the nodes fits in scalar value the same as in Scalar function.\n",
    "\n",
    "- vector value\n",
    "\n",
    "the nodes fits in vector value the same as in Scalar function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"SoftwareOrganization\"></a>\n",
    "## Software Organization \n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "The directory structure will be as follows\n",
    "\n",
    "`\n",
    "AutoDiff\n",
    "|-README.md\n",
    "|-LICENSE\n",
    "|-setup.py\n",
    "|-requirements.txt\n",
    "|-AutoDiff\n",
    "  |-__init__.py\n",
    "  |-variables.py\n",
    "  |-numpy.py\n",
    "  |-user_func.py\n",
    "  |-derivatives.py\n",
    "|-docs\n",
    "  |-documentation.md\n",
    "|-tests\n",
    "  |-__init__.py\n",
    "  |-test_basic.py\n",
    "  |-test_numpy.py\n",
    "  |-test_user_func.py\n",
    "  |-test_derivatives.py\n",
    "`\n",
    "### Modules\n",
    "\n",
    "The `Variables` module contains the functionality to define variables that are compatible with automatic differentiation. These variables will be passed to functions in AutoDiff.Numpy or user defined functions.\n",
    "\n",
    "\n",
    "\n",
    "### Test Suite\n",
    "\n",
    "We will store our tests in the `tests` module and run them using `TravisCI`.\n",
    "\n",
    "### Distribution\n",
    "\n",
    "We will distribute our package on `PyPI`\n",
    "\n",
    ">Todo: Discuss how you plan on organizing your software package.\n",
    "* What will the directory structure look like?  \n",
    "* What modules do you plan on including?  What is their basic functionality?\n",
    "* Where will your test suite live?  Will you use `TravisCI`? `Coveralls`?\n",
    "* How will you distribute your package (e.g. `PyPI`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"implementation\"></a>\n",
    "## Implementation \n",
    "<a id=\"p1\"></a>\n",
    "### Core Data Structure\n",
    "We want to follow the computational graph and construct the \"node\" class as our major data structure `AutoDiff` class. So we can go through the computational graph and update the value and derivatives along the graph.\n",
    "\n",
    "![comp-graph](figs/Computational-Graph.png)\n",
    "\n",
    "<a id=\"p2\"></a>\n",
    "### Major Class\n",
    "The main class that we will implement is the `AutoDiff` class that takes as input the values of the \"independent variables\"(either a scaler or a vector) at the function that we are calculating the derivative on. The \"independent variables\" can be seen as a node in the computational graph. \n",
    "\n",
    "<a id=\"p3\"></a>\n",
    "### Method and Name Attributes in AutoDiff Class\n",
    "* Name Attributes\n",
    "\n",
    "The `AutoDiff` class will have two main instance variables, the value of the AutoDiff instance, and the derivatives of the instance. The derivatives of the instance is initialized with the relevant `n_dim` seed vectors, where `n_dim` is the number of independent variables.\n",
    "\n",
    "1. `AutoDiff.n_dim`: number of the variables in the target function. Use it to decide the dimension of derivatives.\n",
    "2. `AutoDiff.val`: value of variable(nodes). the shape is the same as the input variable. So if input is scalar, it will be scalar, while if input is vector, it is vector.\n",
    "3. `AutoDiff.der`: value of derivatives in this nodes. It is `m*n_dim` array, which store the value of the nodes(`m` value in a vector) to `n_dim` different variables.\n",
    "\n",
    "* Overide the four basic operations. \n",
    "\n",
    "In order to override the four basic operations of elementary arithmetic (addition, subtraction, multiplication, and division), we use dunder methods within our `AutoDiff` class. The dunder methods return new `AutoDiff` instances with the updated value and derivatives.\n",
    "\n",
    "<a id=\"p4\"></a>\n",
    "### Other function \n",
    "\n",
    "* Define elementary differentiation function. \n",
    "\n",
    "In order to deal with the other elementary functions (exponential, logarithm, powers, roots, trigonometric functions, inverse trigonometric functions, hyperbolic functions, etc.), we will override the numpy elementary functions such that we can use it for our AutoDiff class. \n",
    ">For example, we will override the `np.sin` function such that if you use it on an `AutoDiff` instance `x` at a given value, it will return another `AutoDiff` instance with the value of $\\sin(x)$, and the calculated derivative of $\\dot{x}cos(x)$ at the given value. Similarly, we will override the `np.exp` function such that if you use it on an `AutoDiff` instance `x` at a given value, it will return another `AutoDiff` instance with the value of $\\exp(x)$, and the calculated derivative of $\\dot{x}exp(x)$ at the given value.\n",
    "\n",
    "* Define non-differentiable function.\n",
    "\n",
    "We can even handle some function which is non-differentiable at certain points, such as Zigzig function like Brownian Motion, or like $f(x)=\\frac{1}{x}$ at $x=0$. This is our extension for the `AutoDiff` class. We will employ $$f'(x)\\approx\\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$ to calculate the differentiation of that point. <font color = 'red'> **JOSH: ARE WE SURE THAT THIS WILL WORK? DO WE WANT TO ENSURE CONTINUITY AT LEAST?** </font>\n",
    "\n",
    "<a id=\"p5\"></a>\n",
    "### External dependencies \n",
    "\n",
    "In order to implement this, we will rely on the numpy and math external libraries, which will be specified as our external dependencies in our setup.py file.\n",
    "\n",
    "\n",
    "As such, after the user initializes the AutoDiff class on the indepndent variables, he/she will be able to use the usual elementary functions on the AutoDiff instance in order to calculate both the value of the function and the value of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
