{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../automin/autodiff')\n",
    "import variables as v\n",
    "import AD_numpy as anp\n",
    "import vector_variables as vv\n",
    "sys.path.insert(0, '../automin')\n",
    "import optimizer as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Optimization Package with AD functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "Taking derivatives is an essential operation in numerical methods, optimization, and science. From a computational perspective, however, calculating a derivative can be difficult.\n",
    "\n",
    "**Finite Differences** requires careful selection of $\\epsilon$\n",
    "\n",
    "**Symbolic Differentiation** is infeasible for complicated functions, especially for higher order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "**Automatic Differentiation** overcomes these challenges by providing both quick and accurate derivatives.\n",
    "\n",
    "This also allows us to provide optimization methods that are fast and accurate. Most of the standard optimization modules such as `scipy.optimize` do not rely on automatic differentiation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Brief Mathematical Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo for AD module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# user has a given function\n",
    "f1 = lambda x, y: anp.exp(3*x) + anp.log(y/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.83425860084327"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he can calculate its value\n",
    "f1(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# but now he wishes to get its derivative\n",
    "a = v.Variable('a', 2)\n",
    "b = v.Variable('b', 3)\n",
    "res1 = f1(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 403.83425860084327.\n",
      "Partial Derivative wrt a: 1209.7863804782053\n",
      "Jacobian: {'b': 0.3333333333333333, 'a': 1209.7863804782053}\n"
     ]
    }
   ],
   "source": [
    "print('Value: {}.'.format(res1.val))\n",
    "print('Partial Derivative wrt a: {}'.format(res1.partial_der(a)))\n",
    "print('Jacobian: {}'.format(res1.jacobian()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo for AD module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# similarly, if he has a vector function\n",
    "@vv.vectorize_variable\n",
    "def vec_fn(x, y, z):\n",
    "    f1 = x * y + anp.sin(y) + anp.cos(z)\n",
    "    f2 = x + y + anp.sin(x*y)\n",
    "    return np.array([f1,f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# he can extract the values, jacobian, and partial derivatives similarly\n",
    "c = v.Variable('c', 2)\n",
    "res2 = vec_fn(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [3.42532415 4.14112001].\n",
      "Partial Derivative wrt a: [1.        0.0100075]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.540302</td>\n",
       "      <td>-0.909297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010008</td>\n",
       "      <td>-1.969977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c\n",
       "0  1.000000  3.540302 -0.909297\n",
       "1  0.010008 -1.969977  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Value: {}.'.format(res2.val))\n",
    "print('Partial Derivative wrt a: {}'.format(res2.partial_der(a)))\n",
    "display(res2.jacobian())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now all that seems trivial. \n",
    "\n",
    "When would a user just want to solely calculate the gradient of a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Instead, suppose a user wants to find the root of a given function using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def newton_method_scalar(fn, x, threshold, max_iter, verbose=True, norm=2):\n",
    "    \n",
    "    var_names = ['x'+str(idx) for idx in range(len(x))]\n",
    "    \n",
    "    x = np.array(x)\n",
    "    nums_iteration = 0\n",
    "    while True:\n",
    "        x_new = x - fn(*x) / _get_grad(fn, x, var_names)\n",
    "\n",
    "        # print iteration output\n",
    "        if verbose is True:\n",
    "            print(f'Iteration at {nums_iteration}, at {x} ')\n",
    "        \n",
    "        # threshold stopping condition \n",
    "        if np.linalg.norm(x-x_new, norm) < threshold:\n",
    "            print(f'After {nums_iteration} iterations, found a root: {x_new}')\n",
    "            break\n",
    "        \n",
    "        # iteration stopping condition\n",
    "        if nums_iteration >= max_iter:\n",
    "            break\n",
    "        nums_iteration +=1\n",
    "        x = x_new\n",
    "        \n",
    "def _get_grad(fn, x, var_names):\n",
    "    variables = [v.Variable(var_names[idx], x_n) for idx, x_n in enumerate(x)]\n",
    "    out = fn(*variables)\n",
    "    jacobian = out.jacobian()\n",
    "    grad = np.array([jacobian[name] for name in var_names])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration at 0, at [3 2 1] \n",
      "Iteration at 1, at [4.5 3.5 2.5] \n",
      "Iteration at 2, at [3.75 2.75 1.75] \n",
      "Iteration at 3, at [4.125 3.125 2.125] \n",
      "Iteration at 4, at [3.9375 2.9375 1.9375] \n",
      "Iteration at 5, at [4.03125 3.03125 2.03125] \n",
      "Iteration at 6, at [3.984375 2.984375 1.984375] \n",
      "Iteration at 7, at [4.0078125 3.0078125 2.0078125] \n",
      "Iteration at 8, at [3.99609375 2.99609375 1.99609375] \n",
      "Iteration at 9, at [4.00195312 3.00195312 2.00195312] \n",
      "Iteration at 10, at [3.99902344 2.99902344 1.99902344] \n",
      "Iteration at 11, at [4.00048828 3.00048828 2.00048828] \n",
      "Iteration at 12, at [3.99975586 2.99975586 1.99975586] \n",
      "Iteration at 13, at [4.00012207 3.00012207 2.00012207] \n",
      "Iteration at 14, at [3.99993896 2.99993896 1.99993896] \n",
      "Iteration at 15, at [4.00003052 3.00003052 2.00003052] \n",
      "Iteration at 16, at [3.99998474 2.99998474 1.99998474] \n",
      "Iteration at 17, at [4.00000763 3.00000763 2.00000763] \n",
      "Iteration at 18, at [3.99999619 2.99999619 1.99999619] \n",
      "Iteration at 19, at [4.00000191 3.00000191 2.00000191] \n",
      "Iteration at 20, at [3.99999905 2.99999905 1.99999905] \n",
      "Iteration at 21, at [4.00000048 3.00000048 2.00000048] \n",
      "Iteration at 22, at [3.99999976 2.99999976 1.99999976] \n",
      "After 22 iterations, found a root: [4.00000012 3.00000012 2.00000012]\n"
     ]
    }
   ],
   "source": [
    "f2 = lambda x, y, z: (x-4)**2 + (y-3)**2 + (z-2)**2\n",
    "newton_method_scalar(f2, [3, 2, 1], 1e-6, 50, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**It is relatively simple to write up code for algorithms requiring first-order derivatives!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**A common class of problems requiring derivatives is optimization**\n",
    "\n",
    "As such, we decided to use our AD module as a basis for an optimization package!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization\n",
    "\n",
    "Generally, optimization is to find the 'best available' values of an objective function given a defined domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simply, optimization can be thought of as maximizing (or minimizing) a real function**\n",
    "\n",
    "Our optimization module thus seeks to provide users a way to minimize functions. We have implemented 4 first-order iterative algorithms to minimize a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization\n",
    "\n",
    "1. Gradient Descent\n",
    "\n",
    "2. Steepest Descent\n",
    "\n",
    "3. Conjugate Gradient \n",
    "\n",
    "4. Broyden-Fletcher-Goldfarb-Shanno (BFGS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is the simplest first-order iterative algorithm but introduces a key idea that forms the basis of iterative algorithms. \n",
    "\n",
    "**Notice that if we walk in the negative direction of the gradient, it seems like we will eventually reach the minimum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Mathematically, for a function $\\ell(x)$ this is saying that to move from $x^{(t)}$ to $x^{(t+1)}$, we should follow the path of steepest descent $-\\nabla\\ell(x^{(t)})$\n",
    "\n",
    "If we do this over and over again ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/gradient-descent1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/gradient-descent2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figs/gradient-descent3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear that we will eventually reach the minimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, an even better method would be to update the direction with each step\n",
    "\n",
    "This will allow us to pick favorable directions that take better steps or to pick 'non-interfering' directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steepest Descent\n",
    "\n",
    "Steepest descent chooses a new direction through a line minimzation along a direction $s$, before moving along a new direction $-\\nabla f$. \n",
    "\n",
    "![steepestdescent.png](figs/steepestdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This basically means that steepest descent is picking a new direction that is orthogonal to the previous direction\n",
    "\n",
    "\n",
    "![descent.png](figs/descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient\n",
    "\n",
    "After the line minimization along a direction $u$, instead of moving along a direction $v = -\\nabla f$ which is the direction of steepest descent, we can instead perturb the direction to ensure that $\\nabla f$ is perpendicular to the line minimization direction $u$ before and after we move along the perturbed direction.\n",
    "\n",
    "![](figs/conjugategradient.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This ensures that the conjugate gradient method moves in non-interfering directions**\n",
    "\n",
    "In steepest descent, we only make right angle turns. Each step is along directions that can undo the minimization in the previous step. As such, we might take some time before it reaches the minimum. This is often the case in practice.\n",
    "\n",
    "Conjugate gradient instead uses conjugate directions which preserve the minimization achieved in the previous step. \n",
    "\n",
    "![](figs/descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Future Work\n",
    "\n",
    "- Extract Hessian of Jacobian matrices, and 2nd order derivatives to expand optimization methods to include methods such as Newton's method, etc.\n",
    "- Matrix calculus with applications in calculating gradients of complex deep learning networks for loss minimization. Could also be useful in mixture models, such as when using EM algorithm to analyze Gaussian Mixture models.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
