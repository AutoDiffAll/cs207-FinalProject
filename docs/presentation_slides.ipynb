{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../AutoDiff')\n",
    "import variables as v\n",
    "import AD_numpy as anp\n",
    "import vector_variables as vv\n",
    "sys.path.insert(0, '../Implementation')\n",
    "import Optimizer as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Optimization Package with AD functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "Taking derivatives is an essential operation in numerical methods, optimization, and science. From a computational perspective, however, calculating a derivative can be difficult.\n",
    "\n",
    "**Finite Differences** requires careful selection of $\\epsilon$\n",
    "\n",
    "**Symbolic Differentiation** is infeasible for complicated functions, especially for higher order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "**Automatic Differentiation** overcomes these challenges by providing both quick and accurate derivatives.\n",
    "\n",
    "This also allows us to provide optimization methods that are fast and accurate. Most of the standard optimization modules such as `scipy.optimize` do not rely on automatic differentiation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Brief Mathematical Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo for AD module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# user has a given function\n",
    "f1 = lambda x, y: anp.exp(3*x) + anp.log(y/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.83425860084327"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he can calculate its value\n",
    "f1(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# but now he wishes to get it' derivative\n",
    "a = v.Variable('a', 2)\n",
    "b = v.Variable('b', 3)\n",
    "res1 = f1(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 403.83425860084327.\n",
      "Partial Derivative wrt a: 1209.7863804782053\n",
      "Jacobian: {'b': 0.3333333333333333, 'a': 1209.7863804782053}\n"
     ]
    }
   ],
   "source": [
    "print('Value: {}.'.format(res1.val))\n",
    "print('Partial Derivative wrt a: {}'.format(res1.partial_der(a)))\n",
    "print('Jacobian: {}'.format(res1.jacobian()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# similarly, if he has a vector function\n",
    "@vv.vectorize_variable\n",
    "def vec_fn(x, y, z):\n",
    "    f1 = anp.cos(x) + anp.sin(y)+ anp.cos(z) \n",
    "    f2 = x**2 - y**2 - z**2\n",
    "    return np.array([f1,f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# he can extract the values, jacobian, and partial derivatives similarly\n",
    "c = v.Variable('c', 5)\n",
    "res2 = vec_fn(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [ 8.63535698e-03 -3.00000000e+01].\n",
      "Partial Derivative wrt a: [-0.90929743  4.        ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.909297</td>\n",
       "      <td>-0.989992</td>\n",
       "      <td>0.958924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b          c\n",
       "0 -0.909297 -0.989992   0.958924\n",
       "1  4.000000 -6.000000 -10.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Value: {}.'.format(res2.val))\n",
    "print('Partial Derivative wrt a: {}'.format(res2.partial_der(a)))\n",
    "display(res2.jacobian())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now all that seems trivial. \n",
    "\n",
    "When would a user just want to solely calculate the gradient of a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Instead, suppose a user wants to find the root of a given function using Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def newton_method_scalar(fn, x, threshold, max_iter, verbose=True, norm=2):\n",
    "    \n",
    "    var_names = ['x'+str(idx) for idx in range(len(x))]\n",
    "    \n",
    "    x = np.array(x)\n",
    "    nums_iteration = 0\n",
    "    while True:\n",
    "        x_new = x - fn(*x) / _get_grad(fn, x, var_names)\n",
    "\n",
    "        # print iteration output\n",
    "        if verbose is True:\n",
    "            print(f'Iteration at {nums_iteration}, at {x} ')\n",
    "        \n",
    "        # threshold stopping condition \n",
    "        if np.linalg.norm(x-x_new, norm) < threshold:\n",
    "            print(f'After {nums_iteration} iterations, found a root: {x_new}')\n",
    "            break\n",
    "        \n",
    "        # iteration stopping condition\n",
    "        if nums_iteration >= max_iter:\n",
    "            break\n",
    "        nums_iteration +=1\n",
    "        x = x_new\n",
    "        \n",
    "def _get_grad(fn, x, var_names):\n",
    "    variables = [v.Variable(var_names[idx], x_n) for idx, x_n in enumerate(x)]\n",
    "    out = fn(*variables)\n",
    "    jacobian = out.jacobian()\n",
    "    grad = np.array([jacobian[name] for name in var_names])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration at 0, at [3 2 5] \n",
      "Iteration at 1, at [8.5        7.5        3.16666667] \n",
      "Iteration at 2, at [  3.84876543   2.84876543 -14.77380952] \n",
      "Iteration at 3, at [934.21288005 933.21288005  -6.38554121] \n",
      "Iteration at 4, at [3.96220365e+00 2.96220365e+00 1.03186854e+05] \n",
      "Iteration at 5, at [1.40848446e+11 1.40848446e+11 5.15944269e+04] \n",
      "Iteration at 6, at [ 3.99053955e+00  2.99053955e+00 -3.84519315e+17] \n",
      "Iteration at 7, at [ 7.81438071e+36  7.81438071e+36 -1.92259658e+17] \n",
      "Iteration at 8, at [0.00000000e+00 0.00000000e+00 3.17614973e+56] \n",
      "Iteration at 9, at [1.26099089e+112 1.68132118e+112 1.58807486e+056] \n",
      "Iteration at 10, at [-4.90385345e+111  3.67789009e+111 -1.39065829e+168] \n",
      "Iteration at 11, at [ inf -inf  inf] \n",
      "Iteration at 12, at [nan nan nan] \n",
      "Iteration at 13, at [nan nan nan] \n",
      "Iteration at 14, at [nan nan nan] \n",
      "Iteration at 15, at [nan nan nan] \n",
      "Iteration at 16, at [nan nan nan] \n",
      "Iteration at 17, at [nan nan nan] \n",
      "Iteration at 18, at [nan nan nan] \n",
      "Iteration at 19, at [nan nan nan] \n",
      "Iteration at 20, at [nan nan nan] \n",
      "Iteration at 21, at [nan nan nan] \n",
      "Iteration at 22, at [nan nan nan] \n",
      "Iteration at 23, at [nan nan nan] \n",
      "Iteration at 24, at [nan nan nan] \n",
      "Iteration at 25, at [nan nan nan] \n",
      "Iteration at 26, at [nan nan nan] \n",
      "Iteration at 27, at [nan nan nan] \n",
      "Iteration at 28, at [nan nan nan] \n",
      "Iteration at 29, at [nan nan nan] \n",
      "Iteration at 30, at [nan nan nan] \n",
      "Iteration at 31, at [nan nan nan] \n",
      "Iteration at 32, at [nan nan nan] \n",
      "Iteration at 33, at [nan nan nan] \n",
      "Iteration at 34, at [nan nan nan] \n",
      "Iteration at 35, at [nan nan nan] \n",
      "Iteration at 36, at [nan nan nan] \n",
      "Iteration at 37, at [nan nan nan] \n",
      "Iteration at 38, at [nan nan nan] \n",
      "Iteration at 39, at [nan nan nan] \n",
      "Iteration at 40, at [nan nan nan] \n",
      "Iteration at 41, at [nan nan nan] \n",
      "Iteration at 42, at [nan nan nan] \n",
      "Iteration at 43, at [nan nan nan] \n",
      "Iteration at 44, at [nan nan nan] \n",
      "Iteration at 45, at [nan nan nan] \n",
      "Iteration at 46, at [nan nan nan] \n",
      "Iteration at 47, at [nan nan nan] \n",
      "Iteration at 48, at [nan nan nan] \n",
      "Iteration at 49, at [nan nan nan] \n",
      "Iteration at 50, at [nan nan nan] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shengsiong\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "../AutoDiff\\variables.py:131: RuntimeWarning: overflow encountered in double_scalars\n",
      "  pow = binary_user_function(lambda x,y: x**y, lambda x,y: y*(x**(y-1)), lambda x,y: x**y*np.log(x))\n",
      "C:\\Users\\shengsiong\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "f2 = lambda x, y, z: (x-4)**2 + (y-3)**2 + (z-2)**2\n",
    "newton_method_scalar(f2, [3, 2, 5], 1e-6, 50, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
